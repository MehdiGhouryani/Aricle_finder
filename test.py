import requests
from telegram import Update, KeyboardButton, ReplyKeyboardMarkup
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes

import sqlite3
import logging

logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s', level=logging.INFO)
logger = logging.getLogger(__name__)

TOKEN = '7821187888:AAGE4gJs0q5S2-Cbxsz67xU4yMoiY-2aOu4'
CROSSREF_API_URL = 'https://api.crossref.org/works/'
SEMANTIC_SCHOLAR_API_URL = 'https://api.semanticscholar.org/graph/v1/paper/search'
SCIHUB_URL = 'https://sci-hub.se/'

# ØªÙ†Ø¸ÛŒÙ… Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ SQLite
conn = sqlite3.connect('users.db', check_same_thread=False)
cursor = conn.cursor()

# Ø§ÛŒØ¬Ø§Ø¯ Ø¬Ø¯ÙˆÙ„ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ùˆ Ø¢Ù…Ø§Ø±
cursor.execute('''CREATE TABLE IF NOT EXISTS users (id INTEGER PRIMARY KEY, username TEXT, keywords TEXT)''')
cursor.execute('''CREATE TABLE IF NOT EXISTS stats (searches_successful INTEGER, searches_failed INTEGER)''')
conn.commit()








async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user = update.message.from_user
    cursor.execute('INSERT OR IGNORE INTO users (id, username, keywords) VALUES (?, ?, ?)', (user.id, user.username, None))
    conn.commit()

    keyboards = [
        [KeyboardButton('Ø¯Ø±ÛŒØ§ÙØª Ø¨Ø§ DOI'), KeyboardButton('Ø¯Ø±ÛŒØ§ÙØª Ø¨Ø§ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ')],
        [KeyboardButton('Ø¨Ø®Ø´ Ø§Ø±Ø³Ø§Ù„ Ø®ÙˆØ¯Ú©Ø§Ø±')],
    ]
    reply_markup = ReplyKeyboardMarkup(keyboards, resize_keyboard=True)
    await update.message.reply_text('Ø³Ù„Ø§Ù… Ø¨Ù‡ Ø±Ø¨Ø§Øª Ù…Ù‚Ø§Ù„Ù‡ ÛŒØ§Ø¨ Ø®ÙˆØ´ Ø§ÙˆÙ…Ø¯ÛŒÙ†', reply_markup=reply_markup)









async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user_id = update.message.from_user.id
    text = update.message.text
    user_message = update.message.text

    if text == 'Ø¯Ø±ÛŒØ§ÙØª Ø¨Ø§ DOI':
        context.user_data['await_doi'] = True
        context.user_data['await_keywords'] = False 
        await update.message.reply_text('DOI Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:')

    elif context.user_data.get('await_doi') and text not in ['Ø¯Ø±ÛŒØ§ÙØª Ø¨Ø§ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ', 'Ø¨Ø®Ø´ Ø§Ø±Ø³Ø§Ù„ Ø®ÙˆØ¯Ú©Ø§Ø±']:
        if "https://doi.org/" in user_message:
            doi = user_message.split("https://doi.org/")[-1].strip()
        else:    
            doi = user_message
        result = search_in_multiple_sources(doi)
        context.user_data['await_doi'] = False
        await update.message.reply_text(result)

    elif text == 'Ø¯Ø±ÛŒØ§ÙØª Ø¨Ø§ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ':
        context.user_data['await_keywords'] = True
        context.user_data['await_doi'] = False  
        await update.message.reply_text('Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø¯Ù†Ø¸Ø± Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ (Ø¨Ø§ Ú©Ø§Ù…Ø§ Ø¬Ø¯Ø§ Ú©Ù†ÛŒØ¯):')

    elif context.user_data.get('await_keywords') and text not in ['Ø¯Ø±ÛŒØ§ÙØª Ø¨Ø§ DOI', 'Ø¨Ø®Ø´ Ø§Ø±Ø³Ø§Ù„ Ø®ÙˆØ¯Ú©Ø§Ø±']:
        keywords = user_message.replace(',', ' ').split()
        result = search_in_multiple_sources(' AND '.join(keywords))
        context.user_data['await_keywords'] = False
        await update.message.reply_text(result)

    elif text == 'Ø¨Ø®Ø´ Ø§Ø±Ø³Ø§Ù„ Ø®ÙˆØ¯Ú©Ø§Ø±':
        await update.message.reply_text("Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¯Ø± Ø­Ø§Ù„ ØªÙˆØ³Ø¹Ù‡ Ø§Ø³Øª.")





# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù…Ù‚Ø§Ù„Ù‡ Ø§Ø² CrossRef Ø¨Ø§ DOI
def fetch_article_by_doi(doi: str) -> str:
    response = requests.get(f"{CROSSREF_API_URL}{doi}")
    if response.status_code == 200:
        data = response.json()
        title = data['message'].get('title', ['Ø¹Ù†ÙˆØ§Ù†ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯'])[0]
        authors = data['message'].get('author', [])
        
        author_names = []
        for author in authors:
            given = author.get('given', 'Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡')
            family = author.get('family', '')
            author_names.append(f"{given} {family}".strip())
        
        authors_str = ', '.join(author_names) if author_names else 'Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡'
        return f"ğŸ“š Title: {title}\nğŸ‘¨â€ğŸ”¬ Authors: {authors_str}\nğŸ”— DOI: {doi}\nğŸ”— URL: {data['message'].get('URL', 'Ù„ÛŒÙ†Ú©ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª')}"
    else:
        return "Ù…ØªØ§Ø³ÙÙ…ØŒ Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø§ Ø§ÛŒÙ† DOI Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯."
    



# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ù‚Ø§Ù„Ù‡ Ø¯Ø± Sci-Hub Ø¨Ø§ DOI
def fetch_scihub_article(doi: str) -> str:
    url = f"{SCIHUB_URL}{doi}"
    return f"ğŸ“° Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø§ DOI: {doi} Ø¯Ø± Sci-Hub Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª. Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ø§ÛŒÙ† Ù„ÛŒÙ†Ú© Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ù†ÛŒØ¯: {url}"

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ù‚Ø§Ù„Ù‡ Ø¯Ø± Semantic Scholar
def search_articles_by_keywords_scholar(keywords: str) -> str:
    response = requests.get(f"{SEMANTIC_SCHOLAR_API_URL}?query={keywords}&limit=3")
    if response.status_code == 200:
        data = response.json()
        articles = ""
        for paper in data['data']:
            title = paper['title']
            authors = ', '.join(paper['authors'])
            url = paper['url']
            articles += f"ğŸ“š Title: {title}\nğŸ‘¨â€ğŸ”¬ Authors: {authors}\nğŸ”— URL: {url}\n\n"
        return articles if articles else None
    return None

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Google Scholar
def search_articles_by_keywords_google(keywords: str) -> str:
    search_query = scholarly.search_pubs(keywords)
    articles = ""
    for result in search_query:
        title = result['bib']['title']
        authors = result['bib'].get('author', 'Unknown')
        url = result.get('pub_url', 'No URL available')
        articles += f"ğŸ“š Title: {title}\nğŸ‘¨â€ğŸ”¬ Authors: {authors}\nğŸ”— URL: {url}\n\n"
        return articles if articles else None

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± arXiv
def search_arxiv(keywords: str) -> str:
    url = f"http://export.arxiv.org/api/query?search_query=all:{keywords}&start=0&max_results=3"
    response = requests.get(url)
    if response.status_code == 200:
        entries = response.text.split('<entry>')[1:]
        articles = ""
        for entry in entries:
            title = entry.split('<title>')[1].split('</title>')[0]
            authors = entry.split('<name>')[1].split('</name>')[0]
            link = entry.split('<id>')[1].split('</id>')[0]
            articles += f"ğŸ“š Title: {title.strip()}\nğŸ‘¨â€ğŸ”¬ Authors: {authors.strip()}\nğŸ”— URL: {link}\n\n"
        return articles if articles else "No articles found in arXiv."
    return "Error fetching articles from arXiv."


def search_pubmed(keywords: str) -> str:
    url = f"https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pubmed/?format=ris&term={keywords}"
    response = requests.get(url)
    if response.status_code == 200:
        data = response.text
        articles = ""
        for entry in data.split('\n\n'):
            if 'TI  -' in entry and 'AU  -' in entry:
                title = entry.split('TI  - ')[1].split('\n')[0]
                author = entry.split('AU  - ')[1].split('\n')[0]
                articles += f"ğŸ“š Title: {title.strip()}\nğŸ‘¨â€ğŸ”¬ Authors: {author.strip()}\n\n"
        return articles if articles else "No articles found in PubMed."
    return "Error fetching articles from PubMed."



def search_in_multiple_sources(keywords_or_doi: str) -> str:
    if keywords_or_doi.startswith('10.'):
        result = fetch_article_by_doi(keywords_or_doi)

        if 'Ù…ØªØ§Ø³ÙÙ…' in result:
            return result
        

        
        if result:
            cursor.execute('UPDATE stats SET searches_successful = searches_successful + 1')
            conn.commit()
            return result

        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Sci-Hub
        result = fetch_scihub_article(keywords_or_doi)
        if result:
            cursor.execute('UPDATE stats SET searches_successful = searches_successful + 1')
            conn.commit()
            return result



    keywords = ' AND '.join(keywords_or_doi.split(','))

    result = search_articles_by_keywords_scholar(keywords)
    if result:
        cursor.execute('UPDATE stats SET searches_successful = searches_successful + 1')
        conn.commit()
        return result

    result = search_articles_by_keywords_google(keywords)
    if result:
        cursor.execute('UPDATE stats SET searches_successful = searches_successful + 1')
        conn.commit()
        return result

    result = search_arxiv(keywords)
    if result:
        cursor.execute('UPDATE stats SET searches_successful = searches_successful + 1')
        conn.commit()
        return result

    result = search_pubmed(keywords)
    if result:
        cursor.execute('UPDATE stats SET searches_successful = searches_successful + 1')
        conn.commit()
        return result

    cursor.execute('UPDATE stats SET searches_failed = searches_failed + 1')
    conn.commit()
    return "Ù‡ÛŒÚ† Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø´Ù…Ø§ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯."



async def stats(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    cursor.execute('SELECT * FROM stats')
    stats_data = cursor.fetchone()
    if stats_data:
        searches_successful, searches_failed = stats_data
        await update.message.reply_text(f"ğŸ“Š Ø¢Ù…Ø§Ø± Ø¬Ø³ØªØ¬Ùˆ:\nâœ… Ø¬Ø³ØªØ¬ÙˆÙ‡Ø§ÛŒ Ù…ÙˆÙÙ‚: {searches_successful}\nâŒ Ø¬Ø³ØªØ¬ÙˆÙ‡Ø§ÛŒ Ù†Ø§Ù…ÙˆÙÙ‚: {searches_failed}")
    else:
        await update.message.reply_text("Ø¢Ù…Ø§Ø± Ù‡Ù†ÙˆØ² Ø«Ø¨Øª Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.")


def main():
    app = Application.builder().token(TOKEN).build()

    app.add_handler(CommandHandler("start", start))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    app.add_handler(CommandHandler("stats", stats))

    cursor.execute('INSERT OR IGNORE INTO stats (searches_successful, searches_failed) VALUES (0, 0)')
    conn.commit()

    app.run_polling()

if __name__== '__main__':
    main()