import aiohttp
from scholarly import scholarly
from services.scihub_service import fetch_scihub_article

from database import get_connection

CROSSREF_API_URL = 'https://api.crossref.org/works/'
SEMANTIC_SCHOLAR_API_URL = 'https://api.semanticscholar.org/graph/v1/paper/search'



from telegram import Update, Bot
from telegram.ext import ContextTypes
from services.file_service import download_pdf, send_file_to_user
import os

async def handle_doi_request(update: Update, context: ContextTypes.DEFAULT_TYPE,doi):

    user_id = update.message.chat_id
    doi = doi.strip()

    try:
        if not doi.startswith("10."):
            await update.message.reply_text("Ù„Ø·ÙØ§Ù‹ ÛŒÚ© DOI Ù…Ø¹ØªØ¨Ø± ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")
            return

        article_data = await fetch_pdf_link_by_doi(doi)

        if article_data["message"] == "Open Access":
            pdf_link = article_data["pdf_link"]

            file_path = await download_pdf(pdf_link, user_id)


            await send_file_to_user(file_path, user_id, context.bot)

            try:
                os.remove(file_path)
            except Exception as e:
                print(f"Error deleting file {file_path}: {e}")

        else:
            publisher_page = article_data.get("publisher_page", "Ù‡ÛŒÚ† Ù„ÛŒÙ†Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù…Ù‚Ø§Ù„Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            await update.message.reply_text(
                f"Ø§ÛŒÙ† Ù…Ù‚Ø§Ù„Ù‡ Open Access Ù†ÛŒØ³Øª.\nğŸ”— Ù„ÛŒÙ†Ú© ØµÙØ­Ù‡ Ù†Ø§Ø´Ø±: {publisher_page}"
            )

    except Exception as e:
        print(f"Error in handling DOI request: {e}")
        await update.message.reply_text(f"Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯: {e}\nÙ„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.")




# async def fetch_article_by_doi(doi: str) -> str:
#     async with aiohttp.ClientSession() as session:
#         async with session.get(f"{CROSSREF_API_URL}{doi}") as response:
#             if response.status == 200:
#                 data = await response.json()
#                 title = data['message'].get('title', ['Ø¹Ù†ÙˆØ§Ù†ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯'])[0]
#                 authors = data['message'].get('author', [])

#                 author_names = [f"{author.get('given', 'Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡')} {author.get('family', '')}".strip() for author in authors]
#                 authors_str = ', '.join(author_names) if author_names else 'Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡'

#                 pdf_link = data['message'].get('URL', 'Ù„ÛŒÙ†Ú©ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª')
#                 return f"ğŸ“š Ø¹Ù†ÙˆØ§Ù†: {title}\nğŸ‘¨â€ğŸ”¬ Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†: {authors_str}\nğŸ”— DOI: {doi}\nğŸ”— URL: {pdf_link}"

#             return "Ù…ØªØ§Ø³ÙÙ…ØŒ Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø§ Ø§ÛŒÙ† DOI Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯."
        



UNPAYWALL_API_URL = "https://api.unpaywall.org/v2/"
EMAIL_FOR_UNPAYWALL = "mohammadmahdi670@gmail.com"  # Ø§ÛŒÙ…ÛŒÙ„ Ø«Ø¨Øªâ€ŒØ´Ø¯Ù‡ Ø¯Ø± Unpaywall

async def fetch_pdf_link_by_doi(doi: str) -> dict:
    """
    Ø¬Ø³ØªØ¬ÙˆÛŒ DOI Ø¯Ø± Unpaywall Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒÙ†Ú© PDF Ù…Ù‚Ø§Ù„Ù‡
    """
    async with aiohttp.ClientSession() as session:
        async with session.get(f"{UNPAYWALL_API_URL}{doi}?email={EMAIL_FOR_UNPAYWALL}") as response:
            if response.status == 200:
                data = await response.json()
                pdf_link = data.get('best_oa_location', {}).get('url_for_pdf')
                publisher_page = data.get('best_oa_location', {}).get('url')
                return {
                    "pdf_link": pdf_link or None,
                    "publisher_page": publisher_page or None,
                    "message": "Open Access" if pdf_link else "Not Open Access"
                }
            return {"message": "DOI not found"}



async def search_in_multiple_sources(keywords_or_doi: str) -> str:
    
    conn = get_connection()
    cursor = conn.cursor()

    keywords = ' AND '.join(keywords_or_doi.split(','))
    try:
        result = await search_articles_by_keywords_google(keywords)
        if result:
            cursor.execute('UPDATE stats SET searches_successful = searches_successful + 1')
            conn.commit()
            return result
        result = await search_pubmed(keywords)
        if result:
            cursor.execute('UPDATE stats SET searches_successful = searches_successful + 1')
            conn.commit()
            return result
        cursor.execute('UPDATE stats SET searches_failed = searches_failed + 1')
        conn.commit()
        return "Ù‡ÛŒÚ† Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø´Ù…Ø§ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯."
    except Exception as e:
        print(f"ERROR IN SEARCH MULTIPLE SOURCE  ======> {e}")


async def search_articles_by_keywords_google(keywords: str) -> str:
    try:
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ù‚Ø§Ù„Ø§Øª
        search_query = scholarly.search_pubs(keywords)
        
        articles = ""
        max_results = 4  # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬
        count = 0

        for result in search_query:
            if count >= max_results:
                break

            # Ø¹Ù†ÙˆØ§Ù† Ù…Ù‚Ø§Ù„Ù‡
            title = result['bib'].get('title', 'Ø¹Ù†ÙˆØ§Ù†ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯')

            # Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†
            authors_list = result['bib'].get('author', [])
            if authors_list:
                authors = ', '.join(authors_list)
            else:
                authors = "Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù† Ù†Ø§Ø´Ù†Ø§Ø³"

            # Ù„ÛŒÙ†Ú© Ù…Ù‚Ø§Ù„Ù‡
            url = result.get('pub_url')
            if not url:
                url = f"https://www.google.com/search?q={title.replace(' ', '+')}"

            # Ø§ÙØ²ÙˆØ¯Ù† Ø¨Ù‡ Ø®Ø±ÙˆØ¬ÛŒ
            count += 1
            articles += (
                f"ğŸ”¹ Ù…Ù‚Ø§Ù„Ù‡ Ø´Ù…Ø§Ø±Ù‡ {count}:\n"
                f"ğŸ“š Ø¹Ù†ÙˆØ§Ù†: {title}\n"
                f"ğŸ‘¨â€ğŸ”¬ Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†: {authors}\n"
                f"ğŸ”— URL: {url}\n\n"
            )

        return articles if articles else "Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯."

    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ§Ø¨Ø¹ search_articles_by_keywords_google: {e}")
        return "Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯."




async def search_articles_by_keywords(keywords: str) -> str:
    query = '+'.join(keywords.split())
    url = f"{CROSSREF_API_URL}?query={query}&rows=5"
    
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            if response.status == 200:
                data = await response.json()
                articles = ""
                for item in data['message']['items']:
                    title = item['title'][0] if 'title' in item else 'Ø¹Ù†ÙˆØ§Ù†ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯'
                    authors = ', '.join([author['given'] + ' ' + author['family'] for author in item.get('author', [])])
                    url = item.get('URL', 'Ù„ÛŒÙ†Ú©ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª')
                    articles += f"ğŸ“š Ø¹Ù†ÙˆØ§Ù†: {title}\nğŸ‘¨â€ğŸ”¬ Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†: {authors}\nğŸ”— URL: {url}\n\n"
                return articles if articles else "Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯."
            




            
async def search_pubmed(keywords: str) -> str:
    url = f"https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pubmed/?format=ris&term={keywords}"
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            if response.status == 200:
                data = await response.text()
                articles = ""
                for entry in data.split('\n\n'):
                    if 'TI  -' in entry and 'AU  -' in entry:
                        title = entry.split('TI  - ')[1].split('\n')[0]
                        author = entry.split('AU  - ')[1].split('\n')[0]
                        articles += f"ğŸ“š Ø¹Ù†ÙˆØ§Ù†: {title.strip()}\nğŸ‘¨â€ğŸ”¬ Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†: {author.strip()}\n\n"
                return articles if articles else "Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯."




