import aiohttp
from scholarly import scholarly
from services.scihub_service import fetch_scihub_article
from config import send_error_to_admin
from database import get_connection
import xmltodict
import asyncio

CROSSREF_API_URL = 'https://api.crossref.org/works/'
SEMANTIC_SCHOLAR_API_URL = 'https://api.semanticscholar.org/graph/v1/paper/search'



from telegram import Update, Bot
from telegram.ext import ContextTypes
from services.file_service import download_pdf, send_file_to_user
import os

async def handle_doi_request(update: Update, context: ContextTypes.DEFAULT_TYPE,doi):

    user_id = update.message.chat_id
    # doi = doi.strip()
    try:
        if not doi.startswith("10."):
            await update.message.reply_text("Ù„Ø·ÙØ§Ù‹ ÛŒÚ© DOI Ù…Ø¹ØªØ¨Ø± ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")
            return

        article_data = await fetch_pdf_link_by_doi(doi)

        if article_data["message"] == "Open Access":
            pdf_link = article_data["pdf_link"]

            file_path = await download_pdf(pdf_link, user_id)


            await send_file_to_user(file_path, user_id, context.bot)

            try:
                os.remove(file_path)
            except Exception as e:
                print(f"Error deleting file {file_path}: {e}")

        else:
            await update.message.reply_text("Ù…Ù‚Ø§Ù„Ù‡ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª !")
            result = await fetch_article_by_doi(doi)
            await update.message.reply_text(f"{result}\n\n\n Ù…ÛŒØªÙˆÙ†ÛŒ DOI Ø±Ùˆ Ø¨ÙØ±Ø³ØªÛŒ Ø¨Ø®Ø´ Ø®Ù„Ø§ØµÙ‡ Ø³Ø§Ø²ÛŒ Ùˆ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ ÙˆØ§Ø³Øª ÛŒ Ø®Ù„Ø§ØµÙ‡ Ø§Ø²Ø´ Ø¨Ø¯Ù‡ ")

    except Exception as e:
        print(f"Error in handling DOI request: {e}")
        await update.message.reply_text(f"Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯: {e}\nÙ„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯.")




async def fetch_article_by_doi(doi: str) -> str:
    async with aiohttp.ClientSession() as session:
        async with session.get(f"{CROSSREF_API_URL}{doi}") as response:
            try:
                if response.status == 200:
                    data = await response.json()
                    title = data['message'].get('title', ['Ø¹Ù†ÙˆØ§Ù†ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯'])[0]
                    authors = data['message'].get('author', [])

                    author_names = [f"{author.get('given', 'Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡')} {author.get('family', '')}".strip() for author in authors]
                    authors_str = ', '.join(author_names) if author_names else 'Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡'

                    pdf_link = data['message'].get('URL', 'Ù„ÛŒÙ†Ú©ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª')
                    # abstract = data['message'].get('abstract', 'Ú†Ú©ÛŒØ¯Ù‡â€ŒØ§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª')

                    return f"ğŸ“š Ø¹Ù†ÙˆØ§Ù†: {title}\nğŸ‘¨â€ğŸ”¬ Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†: {authors_str}\nğŸ”— DOI: {doi}\nğŸ”— URL: {pdf_link}"

                return "Ù…ØªØ§Ø³ÙÙ…ØŒ Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø§ Ø§ÛŒÙ† DOI Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯."
            except Exception as e:
                error_message = f"Error fetch article by doi  : {str(e)}"
                await send_error_to_admin(error_message)




async def fetch_article_by_doi_for_ai(doi: str) -> str:
    async with aiohttp.ClientSession() as session:
        async with session.get(f"{CROSSREF_API_URL}{doi}") as response:
            try:
                if response.status == 200:
                    data = await response.json()
                    title = data['message'].get('title', ['Ø¹Ù†ÙˆØ§Ù†ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯'])[0]
                    abstract = data['message'].get('abstract', 'Ú†Ú©ÛŒØ¯Ù‡â€ŒØ§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª')

                    return f"ğŸ“š Ø¹Ù†ÙˆØ§Ù†: {title}\nğŸ‘¨â€ğŸ”¬ Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†:\n\nğŸ“ Ú†Ú©ÛŒØ¯Ù‡: {abstract[:600]}"

                return "Ù…ØªØ§Ø³ÙÙ…ØŒ Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø§ Ø§ÛŒÙ† DOI Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯."
            except Exception as e:
                error_message = f"Error fetch article by doi  : {str(e)}"
                await send_error_to_admin(error_message)





UNPAYWALL_API_URL = "https://api.unpaywall.org/v2/"
EMAIL_FOR_UNPAYWALL = "mohammadmahdi670@gmail.com"  # Ø§ÛŒÙ…ÛŒÙ„ Ø«Ø¨Øªâ€ŒØ´Ø¯Ù‡ Ø¯Ø± Unpaywall

async def fetch_pdf_link_by_doi(doi: str) -> dict:
    async with aiohttp.ClientSession() as session:
        async with session.get(f"{UNPAYWALL_API_URL}{doi}?email={EMAIL_FOR_UNPAYWALL}") as response:
            if response.status == 200:
                data = await response.json()
                
                best_oa_location = data.get('best_oa_location', {})
                pdf_link = best_oa_location.get('url_for_pdf')
                publisher_page = best_oa_location.get('url')
                
                return {
                    "pdf_link": pdf_link or None,
                    "publisher_page": publisher_page or None,
                    "message": "Open Access" if pdf_link else "Not Open Access"
                }
            else:
                return {"message": f"Error: {response.status} - DOI not found"}




async def search_in_pubmed_sources(keywords_or_doi: str) -> str:
    conn = get_connection()
    cursor = conn.cursor()
    max_results = 5
    keywords = ' AND '.join(keywords_or_doi.split(','))
    await asyncio.sleep(2)
    try:
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± PubMed
        result = await search_pubmed(keywords, max_results)
        if result:
            cursor.execute('UPDATE stats SET searches_successful = searches_successful + 1')
            conn.commit()
            return result

        cursor.execute('UPDATE stats SET searches_failed = searches_failed + 1')
        conn.commit()
        return "Ù‡ÛŒÚ† Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø´Ù…Ø§ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯."
    
    except Exception as e:
        print(f"ERROR IN SEARCH MULTIPLE SOURCE  ======> {e}")
        return "Ù‡ÛŒÚ† Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø´Ù…Ø§ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯."

            


async def search_in_scholar_sources(keywords_or_doi: str) -> str:
    conn = get_connection()
    cursor = conn.cursor()
    keywords = ' AND '.join(keywords_or_doi.split(','))
    await asyncio.sleep(3)
    try:
        result = await search_articles_by_keywords_google(keywords)
        if result:
            cursor.execute('UPDATE stats SET searches_successful = searches_successful + 1')
            conn.commit()
            return result

        cursor.execute('UPDATE stats SET searches_failed = searches_failed + 1')
        conn.commit()
        return "Ù‡ÛŒÚ† Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø´Ù…Ø§ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯."
    
    except Exception as e:
        print(f"ERROR IN SEARCH MULTIPLE SOURCE  ======> {e}")
        return "Ù‡ÛŒÚ† Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø´Ù…Ø§ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯."





async def search_pubmed(keywords: str, max_results: int = 5) -> str:
    url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
    params = {
        "db": "pubmed",             # Ø¯ÛŒØªØ§Ø¨ÛŒØ³ PubMed
        "term": keywords,           # Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
        "retmax": max_results,      # ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬
        "usehistory": "y",          # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø¬Ø³ØªØ¬Ùˆ
        "retmode": "xml",           # ÙØ±Ù…Øª Ø®Ø±ÙˆØ¬ÛŒ XML
    }
    await asyncio.sleep(2)
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.text()
                    parsed_data = xmltodict.parse(data)
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ù†Ø§Ø³Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ù‚Ø§Ù„Ø§Øª
                    ids = parsed_data['eSearchResult']['IdList']['Id']
                    if isinstance(ids, str):
                        ids = [ids]  # Ø§Ú¯Ø± ÙÙ‚Ø· ÛŒÚ© Ù…Ù‚Ø§Ù„Ù‡ Ø¨Ø§Ø´Ø¯
                    return await fetch_articles(ids)
                else:
                    return "Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ PubMed."
        except Exception as e:
            return f"Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯: {str(e)}"
        

async def fetch_articles(ids: list) -> str:
    url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
    params = {
        "db": "pubmed",
        "id": ",".join(ids),
        "retmode": "xml",
        "rettype": "abstract"
    }
    print(f"fetch_articles: {params}")
    asyncio.sleep(2)
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.text()
                    parsed_data = xmltodict.parse(data)
                    articles = parsed_data['PubmedArticleSet']['PubmedArticle']
                    result = ""

                    # Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ù‚ÛŒÙ‚ ØªØ± Ù…Ù‚Ø§Ù„Ø§Øª Ùˆ Ú†Ø§Ù¾ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
                    for idx, article in enumerate(articles, start=1):
                        title = article['MedlineCitation']['Article']['ArticleTitle']
                        print(f"Article {idx} title: {title}")  # Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¹Ù†ÙˆØ§Ù†
                        
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†
                        authors_list = article['MedlineCitation']['Article'].get('AuthorList', {}).get('Author', [])
                        if isinstance(authors_list, dict):
                            authors = f"{authors_list.get('LastName', '')} {authors_list.get('Initials', '')}"
                        else:
                            authors = ", ".join(
                                f"{author.get('LastName', '')} {author.get('Initials', '')}"
                                for author in authors_list if 'LastName' in author
                            )
                        
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ PMID
                        pmid = article['MedlineCitation']['PMID']['#text']
                        
                        # Ø³Ø§Ø®Øª ÙØ±Ù…Øª Ø®Ø±ÙˆØ¬ÛŒ
                        result += (
                            f"ğŸ”¹ Ù…Ù‚Ø§Ù„Ù‡ Ø´Ù…Ø§Ø±Ù‡ {idx}:\n"
                            f"ğŸ“š Ø¹Ù†ÙˆØ§Ù†: {title}\n"
                            f"ğŸ‘¨â€ğŸ”¬ Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†: {authors}\n"
                            f"ğŸ”— URL: https://pubmed.ncbi.nlm.nih.gov/{pmid}/\n\n"
                        )
                    return result.strip()
                else:
                    return "Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù‚Ø§Ù„Ø§Øª."
        except Exception as e:
            return f"Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯: {str(e)}"



async def search_articles_by_keywords_google(keywords: str) -> str:
    try:
        search_query = scholarly.search_pubs(keywords)
        
        articles = ""
        max_results = 4  
        count = 0
        await asyncio.sleep(2)
        for result in search_query:
            if count >= max_results:
                break

            await asyncio.sleep(3)
            title = result['bib'].get('title', 'Ø¹Ù†ÙˆØ§Ù†ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯')

            authors_list = result['bib'].get('author', [])

            if authors_list:
                authors = ', '.join(authors_list)
            else:
                authors = "Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù† Ù†Ø§Ø´Ù†Ø§Ø³"

            # Ù„ÛŒÙ†Ú© Ù…Ù‚Ø§Ù„Ù‡
            url = result.get('pub_url')
            if not url:
                url = f"https://www.google.com/search?q={title.replace(' ', '+')}"

            # Ø§ÙØ²ÙˆØ¯Ù† Ø¨Ù‡ Ø®Ø±ÙˆØ¬ÛŒ
            count += 1
            articles += (
                f"ğŸ”¹ Ù…Ù‚Ø§Ù„Ù‡ Ø´Ù…Ø§Ø±Ù‡ {count}:\n"
                f"ğŸ“š Ø¹Ù†ÙˆØ§Ù†: {title}\n"
                f"ğŸ‘¨â€ğŸ”¬ Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†: {authors}\n"
                f"ğŸ”— URL: {url}\n\n"
            )

        return articles if articles else "Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯."

    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ§Ø¨Ø¹ search_articles_by_keywords_google: {e}")
        return "Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯."





# async def search_pubmed(keywords: str) -> str:
#     url = f"https://api.ncbi.nlm.nih.gov/lit/ctxp/v1/pubmed/?format=ris&term={keywords}"
#     async with aiohttp.ClientSession() as session:
#         async with session.get(url) as response:
#             if response.status == 200:
#                 data = await response.text()
#                 articles = ""
#                 for entry in data.split('\n\n'):
#                     if 'TI  -' in entry and 'AU  -' in entry:
#                         title = entry.split('TI  - ')[1].split('\n')[0]
#                         author = entry.split('AU  - ')[1].split('\n')[0]
#                         articles += f"ğŸ“š Ø¹Ù†ÙˆØ§Ù†: {title.strip()}\nğŸ‘¨â€ğŸ”¬ Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†: {author.strip()}\n\n"
#                 return articles if articles else "Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯."




# async def search_articles_by_keywords(keywords: str) -> str:
#     query = '+'.join(keywords.split())
#     url = f"{CROSSREF_API_URL}?query={query}&rows=5"
    
#     async with aiohttp.ClientSession() as session:
#         async with session.get(url) as response:
#             if response.status == 200:
#                 data = await response.json()
#                 articles = ""
#                 for item in data['message']['items']:
#                     title = item['title'][0] if 'title' in item else 'Ø¹Ù†ÙˆØ§Ù†ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯'
#                     authors = ', '.join([author['given'] + ' ' + author['family'] for author in item.get('author', [])])
#                     url = item.get('URL', 'Ù„ÛŒÙ†Ú©ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª')
#                     articles += f"ğŸ“š Ø¹Ù†ÙˆØ§Ù†: {title}\nğŸ‘¨â€ğŸ”¬ Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù†: {authors}\nğŸ”— URL: {url}\n\n"
#                 return articles if articles else "Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯."
            



